# GapSight — Social Media Analytics & Competitor Intelligence Dashboard

GapSight is my internship assignment project focused on one simple goal: help a brand quickly understand how it is doing on social media compared to competitors, and what to do next.

I built it end-to-end with a FastAPI backend, React frontend, Supabase database, chart-based analytics, report exports, and an AI insights layer.

## Live Links
- Frontend (Vercel): https://gap-sight.vercel.app/
- Backend (Render): https://gapsight.onrender.com/
- GitHub Repo: https://github.com/hiox2004/GapSight

## Why I Built It This Way
I intentionally used a **mocked-but-realistic dataset** (seeded into Supabase) so I could focus on product thinking, analytics logic, and full-stack quality within the assignment timeline.

### Key decisions and reasoning
- **FastAPI backend**: quick to build clean APIs, easy to structure by feature routers.
- **React + Vite frontend**: fast iteration, clean component-based UI for dashboard cards/charts.
- **Supabase**: practical hosted SQL backend with easy integration for a student project.
- **Seed script + Faker/randomized generation**: gives enough data density for trends, comparisons, and predictions.
- **AI insights layer with strict prompt + fallback rules**: lets insights still work when model/API is unavailable.
- **PDF/CSV report export**: recruiters can quickly see practical output, not just charts.

## Assignment Coverage
### 1) User Social Media Analytics
- Follower growth trends
- Engagement tracking (likes/comments/shares)
- Post performance over time
- Best-performing content type
- Posting frequency vs engagement correlation

### 2) Competitor Analysis Module
- Compare my brand vs competitors
- Competitor follower growth comparison
- Content-type gap analysis
- Performance gap identification

### 3) AI Insight Layer
- What competitors do better
- Content gap suggestions
- Best-time guidance (data-aware recommendations)
- Actionable weekly recommendations

### Bonus Implemented
- PDF and CSV report generation
- Trend prediction (simple linear regression)
- Action playbooks/workflow suggestions

## Mock Data: 

In this project, data is generated by a seeding script (`backend/app/seed.py`) and stored in Supabase. This makes the dashboard realistic enough to test analytics logic while keeping implementation manageable.

Important: the data does **not** auto-refresh by default. It changes only if seeding is run again (or if write endpoints/jobs are added).

## UptimeRobot (Why I Used It)
Render free instances can sleep when inactive, causing first-request cold starts.

I used **UptimeRobot** to periodically ping the backend health endpoint so the API stays warm and the frontend demo feels responsive during evaluation.

- Health endpoint: `/health`
- Purpose: reduce cold-start delays during demo/recruiter testing

## Project Structure
```text
GapSight/
  backend/
    main.py
    app/
      database.py
      models.py
      schemas.py
      seed.py
      routers/
        analytics.py
        competitors.py
        insights.py
        reports.py
  frontend/
    src/
      pages/
      components/
      api/
```

## Quick Local Setup
### 1) Backend
```bash
cd backend
pip install -r requirements.txt
uvicorn main:app --reload
```

### 2) Frontend
```bash
cd frontend
npm install
npm run dev
```

## Environment Variables
Backend needs:
- `SUPABASE_URL`
- `SUPABASE_KEY`
- `GROQ_API_KEY` (optional but recommended for AI insights)

Frontend can use:
- `VITE_API_URL` (defaults to `http://127.0.0.1:8000` locally)

## Seeding Data
From `backend/`:
```bash
python app/seed.py
```

This creates:
- `my_brand` user
- ~90 days of follower metrics
- ~90 days of posts
- 3 competitors + competitor metrics

## What I’d Improve Next
- Add authenticated multi-user support (instead of one fixed `my_brand`)
- Add deterministic seed option for repeatable evaluations
- Add true automation jobs (scheduled tasks), not just action playbooks
- Add tests around analytics calculations and API contracts

## Short Note for Reviewers
This project is built to demonstrate product understanding, practical analytics implementation, and clean full-stack delivery under assignment constraints. I prioritized clarity, end-to-end functionality, and actionable outputs over over-engineering.
